Leave one out cross-validation: we loop through the entire known disease genes list, each time omitting one and running the algorithm with the rest, and then we compare the result (probability/ranking of the omitted disease gene) to the expected result (that it is a disease gene, relatively high probability) then we take the average of the squared differences in actual vs. expected values for each disease gene. This gives us a number that shows how well the algorithm does for a given disease gene set, the lower the better.

Area under ROC curve: we take an output vector from an equation, and pick a bunch of different ranking thresholds. For each threshold, we find the ratios of true positives and false positives etc. above and below the threshold, based on the number of known disease genes that get predicted above and below the threshold. We can then create a data point for each threshold of (TPR, FPR). We then plot all of these data points on an ROC curve and calculate the area under it to measure performance.
